{"cells":[{"cell_type":"markdown","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n","</center>\n","<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn Convolutional Neral Network</h5>\n","<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n"]},{"cell_type":"markdown","metadata":{},"source":["# Table of Contents\n","\n","In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","<li><a href=\"https://#ref0\">Helper functions </a></li>\n","\n","<li><a href=\"https://#ref1\"> Prepare Data </a></li>\n","<li><a href=\"https://#ref2\">Convolutional Neral Network </a></li>\n","<li><a href=\"https://#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n","<li><a href=\"https://#ref4\">Analyse Results</a></li>\n","\n","<br>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref0\"></a>\n","\n","<h2 align=center>Helper functions </h2>\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","import matplotlib.pylab as plt\n","import numpy as np\n","import pandas as pd\n","import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x1aa19f1e8f0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(4)"]},{"cell_type":"markdown","metadata":{},"source":["function to plot out the parameters of the Convolutional layers\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def plot_channels(W):\n","    #number of output channels \n","    n_out=W.shape[0]\n","    #number of input channels \n","    n_in=W.shape[1]\n","    w_min=W.min().item()\n","    w_max=W.max().item()\n","    fig, axes = plt.subplots(n_out,n_in)\n","    fig.subplots_adjust(hspace = 0.1)\n","    out_index=0\n","    in_index=0\n","    #plot outputs as rows inputs as columns \n","    for ax in axes.flat:\n","    \n","        if in_index>n_in-1:\n","            out_index=out_index+1\n","            in_index=0\n","              \n","        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n","        ax.set_yticklabels([])\n","        ax.set_xticklabels([])\n","        in_index=in_index+1\n","\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<code>show_data</code>: plot out data sample\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def show_data(dataset,sample):\n","\n","    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n","    plt.title('y='+str(dataset.y[sample].item()))\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["create some toy data\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","class Data(Dataset):\n","    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n","        \"\"\"\n","        p:portability that pixel is wight  \n","        N_images:number of images \n","        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n","        \"\"\"\n","        if train==True:\n","            np.random.seed(1)  \n","        \n","        #make images multiple of 3 \n","        N_images=2*(N_images//2)\n","        images=np.zeros((N_images,1,11,11))\n","        start1=3\n","        start2=1\n","        self.y=torch.zeros(N_images).type(torch.long)\n","\n","        for n in range(N_images):\n","            if offset>0:\n","        \n","                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n","                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n","            else:\n","                low=4\n","                high=1\n","        \n","            if n<=N_images//2:\n","                self.y[n]=0\n","                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n","            elif  n>N_images//2:\n","                self.y[n]=1\n","                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n","           \n","        \n","        \n","        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n","        self.len=self.x.shape[0]\n","        del(images)\n","        np.random.seed(0)\n","    def __getitem__(self,index):      \n","        return self.x[index],self.y[index]\n","    def __len__(self):\n","        return self.len"]},{"cell_type":"markdown","metadata":{},"source":["<code>plot_activation</code>: plot out the activations of the Convolutional layers\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def plot_activations(A,number_rows= 1,name=\"\"):\n","    A=A[0,:,:,:].detach().numpy()\n","    n_activations=A.shape[0]\n","    \n","    \n","    print(n_activations)\n","    A_min=A.min().item()\n","    A_max=A.max().item()\n","\n","    if n_activations==1:\n","\n","        # Plot the image.\n","        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n","\n","    else:\n","        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n","        fig.subplots_adjust(hspace = 0.4)\n","        for i,ax in enumerate(axes.flat):\n","            if i< n_activations:\n","                # Set the label for the sub-plot.\n","                ax.set_xlabel( \"activation:{0}\".format(i+1))\n","\n","                # Plot the image.\n","                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n","                ax.set_xticks([])\n","                ax.set_yticks([])\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Utility function for computing output of convolutions\n","takes a tuple of (h,w) and returns a tuple of (h,w)\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n","    #by Duane Nielsen\n","    from math import floor\n","    if type(kernel_size) is not tuple:\n","        kernel_size = (kernel_size, kernel_size)\n","    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n","    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n","    return h, w"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref1\"></a>\n","\n","<h2 align=center>Prepare Data </h2> \n"]},{"cell_type":"markdown","metadata":{},"source":["Load the training dataset with 10000 samples\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["N_images=10000\n","train_dataset=Data(N_images=N_images)"]},{"cell_type":"markdown","metadata":{},"source":["Load the testing dataset\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["<__main__.Data at 0x1aa17b98b50>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["validation_dataset=Data(N_images=1000,train=False)\n","validation_dataset"]},{"cell_type":"markdown","metadata":{},"source":["we can see the data type is long\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Visualization\n"]},{"cell_type":"markdown","metadata":{},"source":["Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"]},{"cell_type":"markdown","metadata":{},"source":["We can print out the third label\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["show_data(train_dataset,0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","metadata":{},"source":["we can plot the 3rd  sample\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref3\"></a>\n","\n","### Build a Convolutional Neral Network Class\n"]},{"cell_type":"markdown","metadata":{},"source":["The input image is 11 x11, the following will change the size of the activations:\n","\n","<ul>\n","<il>convolutional layer</il> \n","</ul>\n","<ul>\n","<il>max pooling layer</il> \n","</ul>\n","<ul>\n","<il>convolutional layer </il>\n","</ul>\n","<ul>\n","<il>max pooling layer </il>\n","</ul>\n","\n","with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n","We use the following  lines of code to change the image before we get tot he fully connected layer\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 10)\n","(9, 9)\n","(8, 8)\n","(7, 7)\n"]}],"source":["out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n","print(out)\n","out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n","print(out1)\n","out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n","print(out2)\n","\n","out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n","print(out3)"]},{"cell_type":"markdown","metadata":{},"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class CNN(nn.Module):\n","    def __init__(self,out_1=2,out_2=1):\n","        \n","        super(CNN,self).__init__()\n","        #first Convolutional layers \n","        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n","        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n","\n","        #second Convolutional layers\n","        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n","        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n","        #max pooling \n","\n","        #fully connected layer \n","        self.fc1=nn.Linear(out_2*7*7,2)\n","        \n","    def forward(self,x):\n","        #first Convolutional layers\n","        x=self.cnn1(x)\n","        #activation function \n","        x=torch.relu(x)\n","        #max pooling \n","        x=self.maxpool1(x)\n","        #first Convolutional layers\n","        x=self.cnn2(x)\n","        #activation function\n","        x=torch.relu(x)\n","        #max pooling\n","        x=self.maxpool2(x)\n","        #flatten output \n","        x=x.view(x.size(0),-1)\n","        #fully connected layer\n","        x=self.fc1(x)\n","        return x\n","    \n","    def activations(self,x):\n","        #outputs activation this is not necessary just for fun \n","        z1=self.cnn1(x)\n","        a1=torch.relu(z1)\n","        out=self.maxpool1(a1)\n","        \n","        z2=self.cnn2(out)\n","        a2=torch.relu(z2)\n","        out=self.maxpool2(a2)\n","        out=out.view(out.size(0),-1)\n","\n","        F.s\n","        return z1,a1,z2,a2,out        "]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"ref3\"></a>\n","\n","<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"]},{"cell_type":"markdown","metadata":{},"source":["There are 2 output channels for the first layer, and 1 outputs channel for the second layer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model=CNN(2,1)"]},{"cell_type":"markdown","metadata":{},"source":["we can see the model parameters with the object\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plot_channels(model.state_dict()['cnn1.weight'])\n"]},{"cell_type":"markdown","metadata":{},"source":["Loss function\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","metadata":{},"source":["Define the loss function\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion=nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{},"source":["optimizer class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":["learning_rate=0.001\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["Define the optimizer class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":["\n","train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n","validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"]},{"cell_type":"markdown","metadata":{},"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true},"outputs":[],"source":["n_epochs=10\n","cost_list=[]\n","accuracy_list=[]\n","N_test=len(validation_dataset)\n","cost=0\n","#n_epochs\n","for epoch in range(n_epochs):\n","    cost=0    \n","    for x, y in train_loader:\n","      \n","\n","        #clear gradient \n","        optimizer.zero_grad()\n","        #make a prediction \n","        z=model(x)\n","        # calculate loss \n","        loss=criterion(z,y)\n","        # calculate gradients of parameters \n","        loss.backward()\n","        # update parameters \n","        optimizer.step()\n","        cost+=loss.item()\n","    cost_list.append(cost)\n","        \n","        \n","    correct=0\n","    #perform a prediction on the validation  data  \n","    for x_test, y_test in validation_loader:\n","\n","        z=model(x_test)\n","        _,yhat=torch.max(z.data,1)\n","\n","        correct+=(yhat==y_test).sum().item()\n","        \n","\n","    accuracy=correct/N_test\n","\n","    accuracy_list.append(accuracy)\n","    \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["#### <a id=\"ref3\"></a>\n","\n","<h2 align=center>Analyse Results</h2> \n"]},{"cell_type":"markdown","metadata":{},"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax1 = plt.subplots()\n","color = 'tab:red'\n","ax1.plot(cost_list,color=color)\n","ax1.set_xlabel('epoch',color=color)\n","ax1.set_ylabel('total loss',color=color)\n","ax1.tick_params(axis='y', color=color)\n","    \n","ax2 = ax1.twinx()  \n","color = 'tab:blue'\n","ax2.set_ylabel('accuracy', color=color)  \n","ax2.plot( accuracy_list, color=color)\n","ax2.tick_params(axis='y', labelcolor=color)\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["View the results of the parameters for the Convolutional layers\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn1.weight'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.state_dict()['cnn1.weight']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","metadata":{},"source":["Consider the following sample\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_data(train_dataset,N_images//2+2)"]},{"cell_type":"markdown","metadata":{},"source":["Determine the activations\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n","out=model.activations(train_dataset[0][0].view(1,1,11,11))"]},{"cell_type":"markdown","metadata":{},"source":["Plot them out\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_activations(out[0],number_rows=1,name=\" feature map\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_activations(out[3],number_rows=1,name=\"first feature map\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["we save the output of the activation after flattening\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out1=out[4][0].detach().numpy()"]},{"cell_type":"markdown","metadata":{},"source":["we can do the same for a sample  where y=0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n","out0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.subplot(2, 1, 1)\n","plt.plot( out1, 'b')\n","plt.title('Flatted Activation Values  ')\n","plt.ylabel('Activation')\n","plt.xlabel('index')\n","plt.subplot(2, 1, 2)\n","plt.plot(out0, 'r')\n","plt.xlabel('index')\n","plt.ylabel('Activation')"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"/></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["### About the Authors:\n","\n","[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition.\n","\n","Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2021-01-01)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-23        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n","\n","<hr>\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":2}
